{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはデータの読み込みをした後、すべての出現単語を記録した辞書dicを作成する(辞書といいつつデータフレーム型ですすみません)。  \n",
    "今後必要になるのは\n",
    "- すべてのセリフを格納したserif_all\n",
    "- すべての名前を格納したname_all\n",
    "- すべてのタイプを格納したtype_all\n",
    "- 単語を出現頻度順に並べた辞書のようなものdic(今後は単語をこの\"辞書\"のidに変換して扱う)\n",
    "\n",
    "である、どのような感じかだけつかんでおけばよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  words parts  freq\n",
      "0     、    記号  8202\n",
      "1     …    記号  5063\n",
      "2     ！    記号  4801\n",
      "3     の    助詞  4087\n",
      "4     て    助詞  3968\n",
      "type: cute \n",
      " name: 島村卯月 \n",
      " serif: はじめまして、○○プロデューサーさん！ 島村卯月、17歳です。私、精一杯頑張りますから、一緒に夢叶えましょうね♪よろしくお願いしますっ！\n",
      "type: cute \n",
      " name: 島村卯月+ \n",
      " serif: 〇〇プロデューサーさん、島村卯月です！もう覚えてもらえました？ 私、もっともっと頑張りますから、一緒に夢叶えましょうね♪\n",
      "type: cute \n",
      " name: 中野有香 \n",
      " serif: 押忍！……じゃなかった！お疲れ様ですっ！プロデューサーと言えば師匠も同然！！あたし、きっと最強のアイドルになりますから、ご指導ご鞭撻のほど、よろしくお願いします！！\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import sys\n",
    "from janome.tokenizer import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#セリフのデータを読みこむ\n",
    "types = [\"cute\",\"cool\",\"passion\"]\n",
    "serif_all = []\n",
    "name_all = []\n",
    "type_all = []#0はcute,1はcool,2はpassion\n",
    "for i,type_temp in enumerate(types):#タイプごとにデータを読み込む\n",
    "    filename = \"./data/comment_\"+type_temp+\".txt\"#アイドルコメントの読み込み\n",
    "    file = codecs.open(filename, 'r')\n",
    "    serif_all.extend(file.readlines())\n",
    "    type_all.extend([i]*(len(serif_all)-len(type_all)))#serif_allの長さ-type_allの長さが注目しているタイプのセリフ数\n",
    "    file.close()\n",
    "    filename = \"./data/name_\"+type_temp+\".txt\"#アイドル名の読み込み\n",
    "    file = codecs.open(filename,\"r\")\n",
    "    name_all.extend(file.readlines())\n",
    "    file.close()\n",
    "\n",
    "#品詞ごとに分解する\n",
    "words = [] # 単語文字列、品詞、登場回数のリスト\n",
    "t = Tokenizer()\n",
    "counter = 0\n",
    "for i in range(len(name_all)):#まずは下準備：\"\\n\"を削除\n",
    "    name_all[i] = name_all[i].replace(\"\\n\",\"\")\n",
    "    serif_all[i] = serif_all[i].replace(\"\\n\",\"\")\n",
    "    \n",
    "for serif in serif_all:\n",
    "    counter += 1\n",
    "    print(counter,end = \"\\r\")\n",
    "    tokens = t.tokenize(serif)#品詞ごとに分解される\n",
    "    for j in range(0,len(tokens)): #すべての単語についてみていく\n",
    "        token = tokens[j]#ある単語に注目、jを使わないとうまくいかなかったはず\n",
    "        is_new_word = True\n",
    "        for i in range(len(words)):#既出単語と比較し、tokenが既出か確認\n",
    "            if words[i][0] == token.surface and words[i][1] == token.part_of_speech[:2]:#tokenの単語そのものと品詞が一致していたら既出とみなす\n",
    "                words[i][2] += 1#出現回数を増やす\n",
    "                is_new_word = False#既出だった場合の処理\n",
    "                break\n",
    "        if is_new_word:#新出単語であれば\n",
    "            words.append([token.surface, token.part_of_speech[:2], 1])#wordsに記録、part_of_speech[:2]になっているのはpart_of_speechが\"名詞,一般,*,*\"のような文字列になっているから\"\n",
    "# 単語情報をデータフレームに変換\n",
    "dic = pandas.DataFrame(words)\n",
    "dic.columns = ['words', 'parts', 'freq']#順に単語、品詞、出現頻度の情報\n",
    "dic = dic.sort_values(by=['freq'], ascending=False)\n",
    "dic = dic.reset_index(drop=True)#出現回数の降順にインデックスを付け直す\n",
    "print(dic.head())\n",
    "for i in range(3):#どんなふうになってるか確認\n",
    "    print(\"type:\",types[type_all[i]],\"\\n\",\"name:\",name_all[i],\"\\n\",\"serif:\",serif_all[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に辞書dicのidを利用してコーパスを作成する。このときセリフのつながりなどは全く考慮せずコーパスを作成していることに注意。一応タイプごとには分かれているので、タイプごとの言葉の傾向はとらえているかも?でもあまり期待しないほうが良い。  コーパスを作成する際は単語idのみを利用しているので数字の列にしか見えないことに注意。  \n",
    "ついでにコーパスに現れる単語idから単語への変換を行う辞書id_to_wordと逆の変換を行うword_to_idを作成する。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[882   0   8 ...  26  19   2]\n",
      "146256\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i,serif in enumerate(serif_all):\n",
    "    print(i,end = \"\\r\")\n",
    "    tokens = t.tokenize(serif)\n",
    "    n = len(tokens)\n",
    "    for j in range(n):\n",
    "        dic_temp = dic[(dic['words'] == tokens[j].surface) & (dic['parts'] == tokens[j].part_of_speech[:2])]\n",
    "        corpus.append(dic_temp.index[0])\n",
    "corpus = np.array(corpus)\n",
    "print(corpus)\n",
    "print(len(corpus))\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for i in range(len(dic)):\n",
    "    word_to_id[dic.iloc[i].words] = i\n",
    "    id_to_word[i] = dic.iloc[i].words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOWを利用するためのモジュールをインポートする。ついでにハイパラも設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import config\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.cbow import CBOW\n",
    "from common.util import create_contexts_target,to_cpu,to_gpu\n",
    "#ハイパーパラメータの設定\n",
    "window_size = 5#コンテキストの範囲\n",
    "hidden_size = 100#中間層のサイズ、中間層は1つのみ用いる\n",
    "batch_size = 100\n",
    "max_epoch = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOWを利用して単語の分散表現word_vecsを得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#学習をやり直すときにはこちらを利用する必要がある。\\ntrainer.fit(contexts, target, max_epoch, batch_size)\\ntrainer.plot()\\nword_vecs = model.word_vecs\\nparams = {}\\nparams[\"word_vecs\"] = word_vecs.astype(np.float16)\\npkl_file = \"cbow_params.pkl\"\\nwith open(pkl_file,\"wb\") as f:\\n    pickle.dump(params,f,-1)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "vocab_size  = len(dic)\n",
    "print(vocab_size)\n",
    "contexts,target = create_contexts_target(corpus,window_size)#コーパスからコンテキストとターゲットを生成、これをもとに学習\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)#今回は単純なCBOWを利用\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "pkl_file = \"cbow_params.pkl\"\n",
    "\n",
    "#既存のファイルから情報を取り出すにはこちらを利用する。\n",
    "with open(pkl_file,\"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params[\"word_vecs\"]\n",
    "\"\"\"\n",
    "#学習をやり直すときにはこちらを利用する必要がある。\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "word_vecs = model.word_vecs\n",
    "params = {}\n",
    "params[\"word_vecs\"] = word_vecs.astype(np.float16)\n",
    "pkl_file = \"cbow_params.pkl\"\n",
    "with open(pkl_file,\"wb\") as f:\n",
    "    pickle.dump(params,f,-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語の分散表現を利用して文章(セリフ)も分散表現にする。今回は簡単のため、文章を構成する単語の分散表現の相加平均を取ることとした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35981242 -0.26090834  0.3715744  ...  0.14067854  0.20926793\n",
      "   0.6811693 ]\n",
      " [ 0.20117557 -0.17080596  0.37936771 ...  0.07709448  0.3895948\n",
      "   0.76394098]\n",
      " [ 0.42473611 -0.06763916  0.40240462 ...  0.35961982  0.21317079\n",
      "   0.5450548 ]\n",
      " ...\n",
      " [ 0.14392277  0.07146209  0.21087798 ... -0.01172624  0.22015928\n",
      "   0.28453561]\n",
      " [ 0.23584797  0.29684316  0.32425789 ...  0.0282878   0.3372413\n",
      "   0.26634498]\n",
      " [ 0.13959476  0.28073393  0.35515921 ...  0.31884275  0.0586932\n",
      "   0.24050576]]\n"
     ]
    }
   ],
   "source": [
    "serif_vec = []\n",
    "i = 0\n",
    "for serif in serif_all:#すべてのセリフを分散表現serif_vecに変換する。\n",
    "    print(i,end = \"\\r\")\n",
    "    i += 1\n",
    "    vec_temp = np.zeros(hidden_size)\n",
    "    serif = serif.replace(\"\\n\",\"\")\n",
    "    tokens = t.tokenize(serif)\n",
    "    for token in tokens:\n",
    "        id_temp = word_to_id[token.surface]#単語の文字列からidへ\n",
    "        vec_temp += word_vecs[id_temp]#idから分散表現へ\n",
    "    vec_temp = vec_temp/len(tokens)#単語数で平均をとる\n",
    "    serif_vec.append(vec_temp)\n",
    "serif_vec = np.array(serif_vec)\n",
    "print(serif_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムフォレストによるクラスタリング。入力データは文章の分散表現serif_vec,ターゲットはタイプの正解ラベルtype_allである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yota-\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:accuracy 98.82%\n",
      "accuracy 51.57%\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "idx = np.arange(len(serif_vec))#どのように間違えたかを確認するためにこれが必要になる\n",
    "idx_train,idx_test = sklearn.model_selection.train_test_split(np.arange(len(serif_vec)),test_size = 0.20)\n",
    "train_x = serif_vec[idx_train]\n",
    "train_y = np.array(type_all)[idx_train]\n",
    "test_x = serif_vec[idx_test]\n",
    "test_y = np.array(type_all)[idx_test]\n",
    "model = sklearn.ensemble.RandomForestClassifier()\n",
    "model.fit(train_x, train_y)\n",
    "accuracy = model.score(train_x,train_y)\n",
    "print(\"train:accuracy {0:.2%}\".format(accuracy))\n",
    "accuracy = model.score(test_x, test_y)\n",
    "print('accuracy {0:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サポートベクタマシン(SVM)によるクラスタリング。ランダムフォレストと同じ入力を与える。なぜかわからないがSVMの方が有意に精度はよさそう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:accuracy 60.61%\n",
      "test:accuracy 57.52%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel = \"linear\")\n",
    "model.fit(train_x,train_y)\n",
    "accuracy = model.score(train_x,train_y)\n",
    "print(\"train:accuracy {0:.2%}\".format(accuracy))\n",
    "accuracy = model.score(test_x,test_y)\n",
    "print('test:accuracy {0:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どのようなときに間違えているのかを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 243\n",
      "true: passion pred: cool \n",
      " name: ［学園の魔女］メアリー・コクラン \n",
      " serif: べ、別に怖くないワ！ただの下見だし、何かあっても撮影用の小道具って知ってるモン！それにココが七不思議の現場っていうのは、設定でしょ！…でも、もし何かが出たら…守ってよネ、ダーリン？\n",
      "true: cool pred: passion \n",
      " name: ［ロワイヤルスタイルNP］結城晴+ \n",
      " serif: おっ、いいじゃん、これ！ やっとオレに似合う、まともな衣装がきたっつーか。こういうので踊って歌えっていうなら、やってやってもいいぜ！ ○○、オレのカッコイイところ、見てろよな！\n",
      "true: cute pred: passion \n",
      " name: ［ホームメイドハッピー］五十嵐響子+ \n",
      " serif: みんなっ！今日は、おしとやかな私、卒業ですっ！ハジけるときは、ちゃんとハジけなきゃ♪ファンのみんなも、今日はおもいっきりハジけてくださいねっ！あっ、でも隣の人と仲良く！楽しんで！\n",
      "true: passion pred: cool \n",
      " name: ［海風の使者］浜口あやめ+ \n",
      " serif: 忍びとして自然を味方につけることは心得ております。そう、つまり大海原であろうとあやめに隙はありません！伊賀の里での修行の成果を海軍の提督になりきって披露いたしましょう！\n",
      "true: cute pred: passion \n",
      " name: ［天下御免☆ガール］丹羽仁美 \n",
      " serif: やぁやぁ我こそは丹羽仁美！特売うなぎの蒲焼、勝ち取ったりぃ～肉厚で美味しそうでしょ、○○プロデューサー☆これで故郷の名物、ヒツマブシ作ったげるから、楽しみにしててよ！\n"
     ]
    }
   ],
   "source": [
    "type_pred = model.predict(test_x)#testデータに対してprediction\n",
    "idx_error = idx_test[type_pred != np.array(type_all)[idx_test]]#testデータのうち間違えていたもののindexを抽出\n",
    "print(len(type_pred),len(idx_error))\n",
    "type_pred_all = model.predict(serif_vec)#全体のデータに対してprediction(このあとのために仕方なく使っている)\n",
    "for i,idx_temp in enumerate(idx_error):\n",
    "    if i > 4:\n",
    "        break\n",
    "    print(\"true:\",types[type_all[idx_temp]],\"pred:\",types[type_pred_all[idx_temp]],\"\\n\",\"name:\",name_all[idx_temp],\"\\n\",\"serif:\",serif_all[idx_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混同行列を計算する。行が本当のタイプを表しており、列が予測されたタイプを表している。cuteの分類は難しいらしい。passionに分類されがち。半面、coolやpassionの分類は精度が高く、特にcoolとpassionを混同することは少ない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87.  46.  69.]\n",
      " [ 39. 110.  27.]\n",
      " [ 33.  29. 132.]]\n",
      "[[0.43069307 0.22772277 0.34158416]\n",
      " [0.22159091 0.625      0.15340909]\n",
      " [0.17010309 0.14948454 0.68041237]]\n"
     ]
    }
   ],
   "source": [
    "confusion = np.zeros([3,3])\n",
    "for i,idx_temp in enumerate(idx_test):\n",
    "    confusion[type_all[idx_temp]][type_pred[i]] += 1\n",
    "print(confusion)#混同行列の計算\n",
    "for i in range(len(confusion)):#\n",
    "    confusion[i] = confusion[i]/np.sum(confusion[i])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
